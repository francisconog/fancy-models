{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge Regression\n",
    "\n",
    "The explaination of this algorithm was taken from the book *The Elements of Statistical Learning*, chapter 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from os.path import join\n",
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin, RegressorMixin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explaining the Algorithm\n",
    "\n",
    "Ridge Regression shrinks the regression coefficients by imposing a penalty on their size. The ridge coefficients minimize a penalized residual sum of squares, given by the following equation:\n",
    "\n",
    "$$ \\hat{\\beta}^{lasso} = \\argmin_{\\beta} \\left\\{ \\frac{1}{2} \\sum_{i=1}^{N} \\left( y_i - \\beta_0 - \\sum_{j=1}^{p} x_{ij} \\beta_j \\right)^2 + \\lambda \\sum_{j=1}^{p} \\beta_j^2 \\right\\} $$\n",
    "\n",
    "It can also be wrote in the matrix form:\n",
    "\n",
    "$$ RSS(\\lambda) = (\\bold{y} - \\bold{X} \\beta)^T (\\bold{y} - \\bold{X}  \\beta) + \\lambda \\beta^T \\beta$$\n",
    "\n",
    "Denote by $\\bold{X}$ the $N \\times p $ (not $p + 1$) matrix with each row an input vector, and similarly let $\\bold{y}$ be the N-vector of outputs in the training set. In this case we **DO NOT WANT TO REGULARIZE THE INTERCEPT**, so usually we do not add one extra column with ones. Penalization of the intercept would make the procedure depend on the origin chosen for $\\bold{y}$; that is, adding a constant $c$ to each of the targets $y_i$ would not simply result in a shift of the predictions by the same amount $c$. To find a solution to this problem, one can *center* the inputs around the column mean and update its values to $x_{ij} = x_{ij} - \\overline{x}_j$ and the outputs to $y_{i} = y_{i} - \\overline{y}$. This way the intercept $\\beta_0$ is equal to $\\overline{y} = \\sum_{i=1}^{N} y_i$\n",
    "\n",
    "\n",
    "One way to minimize this function is by setting it's derivative in respect to $\\beta$ to zero.\n",
    "\n",
    "<!-- $$ \\frac{\\partial RSS}{\\partial\\beta}  = -2\\bold{X}^T (\\bold{y} - \\bold{X}  \\beta)$$\n",
    "$$ \\bold{X}^T (\\bold{y} - \\bold{X}  \\beta) = 0 $$ -->\n",
    "\n",
    "$$ \\hat{\\beta} =  (\\bold{X}^T \\bold{X} + \\lambda \\bold{I})^{-1} \\bold{X}^T \\bold{y}$$\n",
    "\n",
    "Other solution for the case that there is an intercept can be found [here](https://stats.stackexchange.com/questions/602412/what-would-be-the-solution-of-ridge-regression-if-there-is-an-intercept). In this case one should include a column of 1s plus any \"features\" or \"independent variables to $\\bold{X}$ and consider the following\n",
    "$$\\gamma = \\begin{bmatrix}\n",
    "\\beta_0 \\\\\n",
    "\\beta \n",
    "\\end{bmatrix}$$\n",
    "\n",
    "$$\\hat{\\gamma} =  (\\bold{X}^T \\bold{X} + \\lambda \\bold{A})^{-1} \\bold{X}^T \\bold{y}$$\n",
    "\n",
    "$$\\bold{A} = \\begin{bmatrix}\n",
    "0 & 0 \\\\\n",
    "0 & \\bold{I} \n",
    "\\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.5\n",
    "X = np.array([[1], [2], [3], [4]])\n",
    "y = np.array([1, 3, 2, 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution that include a column of 1s and do not penalizes the intercept\n",
    "\n",
    "# Adding a new column with ones to represent the intercept\n",
    "_X = np.hstack((np.ones([X.shape[0],1], X.dtype), X))\n",
    "\n",
    "# This \"Turns off\" the regularization for for beta_0\n",
    "A = np.identity(_X.shape[1])\n",
    "A[0, 0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.25, 1.  ])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = np.linalg.inv(_X.T @ _X + alpha * A) @ _X.T @ y\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.25, 2.25, 3.25, 4.25])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_X @ weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution that scales the inputs and outputs\n",
    "\n",
    "# Centering X around the mean\n",
    "X_scaled = X - np.mean(X, axis=0)\n",
    "y_scaled = y - np.mean(y, axis=0)\n",
    "w0 = np.mean(y, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.75, 1.  ])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "I = np.identity(X_scaled.shape[1])\n",
    "\n",
    "weights = np.linalg.inv(X_scaled.T @ X_scaled + alpha * I) @ X_scaled.T @ y_scaled\n",
    "np.hstack([w0, weights])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.25, 2.25, 3.25, 4.25])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_scaled  @ weights + w0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RidgeRegressor(BaseEstimator, RegressorMixin):\n",
    "    def __init__(self, alpha=0.5, scale=False):\n",
    "        self.alpha = alpha\n",
    "        self.scale = scale\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        if self.scale == True:\n",
    "            X_scaled = X - np.mean(X, axis=0)\n",
    "        else:\n",
    "            X_scaled = X.copy()\n",
    "        \n",
    "        self.X = np.hstack((np.ones([X.shape[0],1], X.dtype), X))\n",
    "        self.y = y\n",
    "        self.N, self.p = X.shape\n",
    "\n",
    "        A = np.identity(self.p+1)\n",
    "        \n",
    "        # This \"Turns off\" the regularization for for beta_0\n",
    "        A[0, 0] = 0\n",
    "        \n",
    "        self.weights = np.linalg.inv(self.X.T @ self.X + self.alpha*A) @ self.X.T @ self.y\n",
    "        \n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        _X = np.hstack((np.ones([X.shape[0],1], X.dtype), X))\n",
    "        return _X @ self.weights\n",
    "    \n",
    "    def get_variance(self):\n",
    "        y_hat = self.predict(self.X)\n",
    "        return 1/(self.N - self.p - 1) * np.sum((y_hat - self.y)**2)\n",
    "    \n",
    "    def get_params_covariance(self) -> np.ndarray:\n",
    "        \"\"\"The variance–covariance matrix of the least squares parameter estimates\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: The variance–covariance matrix\n",
    "        \"\"\"\n",
    "        return np.linalg.inv(self.X.T @ self.X) * self.get_variance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.25, 2.25, 3.25, 4.25])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_ridge = RidgeRegressor(alpha=0.5).fit(X, y)\n",
    "y_hat = my_ridge.predict(X)\n",
    "y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.25, 2.25, 3.25, 4.25])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sk_ridge = Ridge(alpha=0.5).fit(X, y)\n",
    "y_hat = sk_ridge.predict(X)\n",
    "y_hat"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
