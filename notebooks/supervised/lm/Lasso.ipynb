{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lasso Regression\n",
    "\n",
    "The explaination of this algorithm was taken from the book *The Elements of Statistical Learning*, chapter 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from os.path import join\n",
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin, RegressorMixin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explaining the Algorithm\n",
    "\n",
    "Lasso Regression shrinks the regression coefficients by imposing a penalty on their size. The ridge coefficients minimize a penalized residual sum of squares, given by the following equation:\n",
    "\n",
    "$$ \\hat{\\beta}^{lasso} = \\argmin_{\\beta} \\left\\{ \\frac{1}{2} \\sum_{i=1}^{N} \\left( y_i - \\beta_0 - \\sum_{j=1}^{p} x_{ij} \\beta_j \\right)^2 + \\lambda \\sum_{j=1}^{p}\\left| \\beta_j \\right| \\right\\} $$\n",
    "\n",
    "It can also be wrote in the matrix form:\n",
    "\n",
    "$$ RSS(\\lambda) = (\\bold{y} - \\bold{X} \\beta)^T (\\bold{y} - \\bold{X}  \\beta) + \\lambda \\beta^T \\beta$$\n",
    "\n",
    "Due to the constraint $\\left| \\beta_j \\right|$ the solutions are nonlinear in the $y_i$, and there is no closed form expression as in ridge regression. The  Lasso algorithm is a quadratic programming problem, so we can use some famous optimizations algorithms such as Gradient Descent and Coordinate Descent. In this code I will implement a Coordinate Descent Algorthm to compute the Lasso Solution. For a more detailed explanaition of the algorithm. See the references. [[1]](https://stats.stackexchange.com/questions/347796/coordinate-descent-for-lasso), [[2]](https://www.stat.cmu.edu/~ryantibs/convexopt/lectures/coord-desc.pdf).\n",
    "\n",
    "\n",
    "### Mathematical Steps in Coordinate Descent\n",
    "1. Start with initial coefficients: Initialize $\\beta=0$\n",
    "\n",
    "2. Iterative updates: For each coefficient $\\beta_j$\n",
    "    - Compute the partial residual:\n",
    "        $$r_j = y - \\sum_{k \\ne j} X_k\\beta_k$$\n",
    "    - Compute the \"soft-thresholded\" update:\n",
    "        $$\\beta_j = soft-threshold\\left( \\frac{X_j^{\\intercal}r_j}{n}, \\frac{\\lambda}{n} \\right)$$\n",
    "    where the soft-thresholding operator is defined as:\n",
    "        $$soft-threshold(z,\\gamma) = \\begin{cases}\n",
    "   z-\\gamma &\\text{if } z \\gt \\gamma \\\\\n",
    "   z+\\gamma &\\text{if } z \\lt -\\gamma \\\\\n",
    "   0 &\\text{if } \\left| z \\right| \\le \\gamma\n",
    "\\end{cases} $$\n",
    "\n",
    "3. Repeat: Iterate over all coefficients until convergence, i.e., until the updates become smaller than a tolerance threshold.\n",
    "\n",
    "There is another extra step for updating the Intercept. For the model to fit well. The residuals should have a mean of zero $(mean(r) = 0)$. This ensures that the predictions are unbiased, i.e., the average prediction matches the average observed value of $y$.\n",
    "\n",
    "#### Intercept Update Formula\n",
    "The intercept is updated to realign the residuals by minimizing their mean:\n",
    " $$\\beta_0 = mean(y) - mean(X\\beta)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def soft_threshold(z, gamma):\n",
    "    if z > gamma:\n",
    "        return z - gamma\n",
    "    elif z < gamma:\n",
    "        return z + gamma\n",
    "    elif abs(z) < gamma:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.5\n",
    "max_iter = 10000\n",
    "X = np.array([[1], [2], [3], [4]])\n",
    "y = np.array([1, 3, 2, 5])\n",
    "\n",
    "# Centering Y for not having to calculate the intercept\n",
    "y_mean = np.mean(y)\n",
    "y_centered = y-y_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.25])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N, p = X.shape\n",
    "weights = np.zeros(p)\n",
    "\n",
    "for i in range(max_iter):\n",
    "    for j in range(p):\n",
    "        r_j = y_centered - X @ weights + X[:, j] * weights[j]\n",
    "        rho_j = X[:, j].T @ r_j\n",
    "        weights[j] = soft_threshold(rho_j/N, alpha/N)\n",
    "\n",
    "weights "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_0 = y_mean - np.mean(X @ weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.5"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean([5,6,7,8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LassoRegressor(BaseEstimator, RegressorMixin):\n",
    "    def __init__(self, alpha=0.5, max_iter=10000, tol=0.01):\n",
    "        self.alpha = alpha\n",
    "        self.max_iter = max_iter\n",
    "        self.tol = tol\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # Centering Y for not having to calculate the intercept\n",
    "        self.y = y.copy()\n",
    "        self.X = X.copy()\n",
    "\n",
    "        y_mean = np.mean(y)\n",
    "        self.y = self.y-y_mean\n",
    "\n",
    "        self.N, self.p = self.X.shape\n",
    "        self.weights = np.zeros(self.p)\n",
    "        prev_weights = np.ones(self.p)\n",
    "\n",
    "        for i in range(self.max_iter):\n",
    "            if np.linalg.norm(self.weights - prev_weights) < self.tol:\n",
    "                break\n",
    "\n",
    "            # Update previous weights\n",
    "            prev_weights = self.weights\n",
    "\n",
    "            for j in range(self.p):\n",
    "                # Calculate the residuals\n",
    "                r_j = self.y - self.X @ self.weights + self.X[:, j] * self.weights[j]\n",
    "                rho_j = self.X[:, j].T @ r_j\n",
    "                self.weights[j] = self._soft_threshold(rho_j/self.N, alpha/self.N)\n",
    "        \n",
    "        \n",
    "        self.b_0 = y_mean - np.mean(X @ self.weights)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        return X @ self.weights + self.b_0\n",
    "    \n",
    "    def _soft_threshold(self, z, gamma):\n",
    "        if z > gamma:\n",
    "            return z - gamma\n",
    "        elif z < gamma:\n",
    "            return z + gamma\n",
    "        elif abs(z) < gamma:\n",
    "            return 0\n",
    "        \n",
    "    def get_variance(self):\n",
    "        y_hat = self.predict(self.X)\n",
    "        return 1/(self.N - self.p - 1) * np.sum((y_hat - self.y)**2)\n",
    "    \n",
    "    def get_params_covariance(self) -> np.ndarray:\n",
    "        \"\"\"The variance–covariance matrix of the least squares parameter estimates\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: The variance–covariance matrix\n",
    "        \"\"\"\n",
    "        return np.linalg.inv(self.X.T @ self.X) * self.get_variance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.875, 2.125, 3.375, 4.625])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_lasso = LassoRegressor(alpha=0.5).fit(X, y)\n",
    "y_hat = my_lasso.predict(X)\n",
    "y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1.25]), -0.375)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_lasso.weights, my_lasso.b_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.7, 2.4, 3.1, 3.8])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sk_lasso = Lasso(alpha=alpha).fit(X, y)\n",
    "y_hat = sk_lasso.predict(X)\n",
    "y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.7]), 1.0)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sk_lasso.coef_, sk_lasso.intercept_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
